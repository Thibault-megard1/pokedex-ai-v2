# üß† Guide d'Installation de l'IA Locale (Ollama)

Ce guide explique comment installer et configurer **Ollama** pour utiliser des mod√®les d'IA localement et **gratuitement** dans le Pok√©dex AI.

## üéØ Pourquoi Ollama ?

- **100% GRATUIT** - Aucun co√ªt d'API, aucune limite d'utilisation
- **Priv√©** - Vos donn√©es restent sur votre machine
- **Rapide** - Pas de latence r√©seau, ex√©cution locale
- **Hors ligne** - Fonctionne sans connexion internet (apr√®s installation)
- **Mod√®les puissants** - Mistral, Llama 3, et plus

## üì• Installation

### Windows

1. **T√©l√©charger Ollama**
   - Visite: https://ollama.ai/download
   - Clique sur "Download for Windows"
   - Lance l'installateur `.exe`

2. **V√©rifier l'installation**
   - Ouvre PowerShell ou Terminal
   - Ex√©cute:
   ```powershell
   ollama --version
   ```
   - R√©sultat attendu: `ollama version X.X.X`

### macOS

**Option 1: Installateur**
1. Visite: https://ollama.ai/download
2. T√©l√©charge le `.dmg` pour Mac
3. Ouvre le fichier et d√©place Ollama vers Applications

**Option 2: Homebrew**
```bash
brew install ollama
```

**V√©rification:**
```bash
ollama --version
```

### Linux

```bash
# Installation automatique
curl -fsSL https://ollama.ai/install.sh | sh

# V√©rification
ollama --version
```

## ü§ñ T√©l√©charger un Mod√®le

Une fois Ollama install√©, t√©l√©charge un mod√®le d'IA:

### Recommand√©: Mistral (7B)
- **Taille**: ~4.1 GB
- **M√©moire requise**: 8 GB RAM
- **Performances**: Rapide, excellent fran√ßais
- **Id√©al pour**: Quiz personnalit√©, suggestions Pok√©mon

```bash
ollama pull mistral
```

### Alternative: Llama 3 (8B)
- **Taille**: ~4.7 GB
- **M√©moire requise**: 8 GB RAM
- **Performances**: Meilleur raisonnement
- **Id√©al pour**: Analyses complexes

```bash
ollama pull llama3
```

### Mod√®les plus petits (si RAM limit√©e)

```bash
# Mistral Nemo (3B) - Plus l√©ger
ollama pull mistral-nemo

# Gemma 2B - Tr√®s l√©ger
ollama pull gemma:2b
```

### Lister les mod√®les install√©s

```bash
ollama list
```

R√©sultat attendu:
```
NAME                ID              SIZE      MODIFIED
mistral:latest      abc123def456    4.1 GB    2 minutes ago
```

## ‚úÖ V√©rifier qu'Ollama Fonctionne

### 1. V√©rifier le service

```bash
# Tester l'API
curl http://localhost:11434/api/tags

# R√©sultat attendu: JSON avec liste des mod√®les
```

Exemple de r√©ponse:
```json
{
  "models": [
    {
      "name": "mistral:latest",
      "modified_at": "2024-01-27T10:30:00Z",
      "size": 4109865216
    }
  ]
}
```

### 2. Test interactif

```bash
ollama run mistral
```

Tu peux alors discuter avec le mod√®le directement dans le terminal:
```
>>> Bonjour, qui es-tu ?
Je suis Mistral, un mod√®le d'intelligence artificielle...

>>> /bye  (pour quitter)
```

### 3. Test via le Pok√©dex AI

1. Configure `.env.local`:
```env
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral
```

2. Red√©marre le serveur:
```bash
npm run dev
```

3. Visite: http://localhost:3000/api/ai/health

R√©sultat attendu:
```json
{
  "success": true,
  "provider": "ollama",
  "status": {
    "provider": "ollama",
    "status": "online",
    "message_fr": "Ollama est lanc√© et pr√™t",
    "model": "mistral",
    "response_time_ms": 45
  }
}
```

4. V√©rifie l'indicateur dans la navbar:
   - Barre de navigation en haut
   - Cherche "IA: En ligne" (pastille verte)

## üîß Configuration Avanc√©e

### Changer le mod√®le

Dans `.env.local`:
```env
# Utiliser Llama 3 au lieu de Mistral
OLLAMA_MODEL=llama3

# Ou un mod√®le sp√©cifique avec tag
OLLAMA_MODEL=mistral:7b-instruct-v0.2
```

### Utiliser un serveur distant

Si Ollama tourne sur une autre machine:
```env
OLLAMA_BASE_URL=http://192.168.1.100:11434
```

### M√©moire GPU (optionnel)

Pour acc√©l√©rer avec GPU NVIDIA:
```bash
# Ollama d√©tecte automatiquement le GPU
# Aucune config suppl√©mentaire requise

# V√©rifier l'utilisation GPU
nvidia-smi  # Si GPU utilis√©, tu verras ollama dans la liste
```

## üêõ D√©pannage

### Probl√®me: "Ollama n'est pas lanc√©"

**Solution 1: D√©marrer Ollama manuellement**
```bash
# Windows
ollama serve

# Mac/Linux
ollama serve
```

**Solution 2: V√©rifier le service**
- **Windows**: Cherche "Ollama" dans les applications en cours
- **Mac**: Cherche l'ic√¥ne Ollama dans la barre de menu
- **Linux**: 
  ```bash
  systemctl status ollama
  sudo systemctl start ollama
  ```

### Probl√®me: "Model not found"

```bash
# T√©l√©charger le mod√®le
ollama pull mistral

# V√©rifier qu'il est install√©
ollama list
```

### Probl√®me: Connexion refus√©e (port 11434)

```bash
# V√©rifier qu'Ollama √©coute sur le bon port
netstat -an | grep 11434  # Linux/Mac
netstat -an | findstr 11434  # Windows

# Red√©marrer Ollama
ollama serve
```

### Probl√®me: Trop lent / RAM insuffisante

**Solutions:**
1. Utilise un mod√®le plus petit:
   ```bash
   ollama pull gemma:2b
   ```
   Dans `.env.local`:
   ```env
   OLLAMA_MODEL=gemma:2b
   ```

2. Ferme les applications inutiles

3. V√©rifie la m√©moire disponible:
   ```bash
   # Linux/Mac
   free -h
   
   # Windows (PowerShell)
   Get-WmiObject Win32_OperatingSystem | Select-Object FreePhysicalMemory
   ```

### Probl√®me: R√©ponses en anglais au lieu du fran√ßais

Le prompt est d√©j√† en fran√ßais dans le code. Si tu veux forcer:

Dans `app/api/quiz/analyze/route.ts`, le system prompt commence par:
```typescript
const systemPrompt = `Tu es un analyste de personnalit√© Pok√©mon expert...`
```

Assure-toi que le mod√®le est bien Mistral (meilleur support fran√ßais):
```env
OLLAMA_MODEL=mistral
```

## üìä Comparaison Ollama vs Mistral API

| Crit√®re | Ollama (Local) | Mistral API (Cloud) |
|---------|----------------|---------------------|
| **Co√ªt** | 0‚Ç¨ (gratuit) | ~0.15‚Ç¨/100 quiz |
| **Vitesse** | 2-5s (local) | 1-3s (r√©seau) |
| **Limite** | Illimit√© | 5 quiz/min |
| **Connexion** | Fonctionne hors ligne | Internet requis |
| **Setup** | Installer Ollama | Juste API key |
| **RAM** | 8GB recommand√© | Aucune |
| **Confidentialit√©** | 100% priv√© | Donn√©es sur serveur |

## üéØ Commandes Utiles

```bash
# Lister les mod√®les install√©s
ollama list

# T√©l√©charger un mod√®le
ollama pull <model-name>

# Supprimer un mod√®le (lib√©rer espace)
ollama rm mistral

# Voir les infos d'un mod√®le
ollama show mistral

# Test interactif
ollama run mistral

# Arr√™ter Ollama
# Windows: Fermer l'app
# Linux: sudo systemctl stop ollama
```

## üìö Ressources

- **Site officiel**: https://ollama.ai
- **Documentation**: https://github.com/ollama/ollama/blob/main/docs/README.md
- **Mod√®les disponibles**: https://ollama.ai/library
- **Discord Ollama**: https://discord.gg/ollama

## ‚úÖ Checklist Finale

- [ ] Ollama install√© (`ollama --version` fonctionne)
- [ ] Mod√®le t√©l√©charg√© (`ollama list` montre mistral)
- [ ] Service actif (`curl http://localhost:11434/api/tags` retourne JSON)
- [ ] `.env.local` configur√© avec `LLM_PROVIDER=ollama`
- [ ] Serveur dev red√©marr√© (`npm run dev`)
- [ ] Health check OK (http://localhost:3000/api/ai/health)
- [ ] Indicateur vert dans la navbar
- [ ] Quiz fonctionne (teste avec un quiz personnalit√©)

F√©licitations ! üéâ Ton Pok√©dex utilise maintenant l'IA locale gratuite !
